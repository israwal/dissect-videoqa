{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#QUAG"
      ],
      "metadata": {
        "id": "av9U2mwdALDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "jvFkxdKkcGYG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LE0z6y8Ub8Vq"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionWithQUAG(nn.Module):\n",
        "  def __init__(self, dim_model):\n",
        "    super().__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.q_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.k_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.v_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.out_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "  def apply_attention_scores(self, attention_scores, value):\n",
        "      attended_output = torch.matmul(attention_scores, value)\n",
        "      return attended_output\n",
        "\n",
        "  def get_rowwise_average(self, scores, mask):\n",
        "    rowwise_sum = scores.sum(-1) #torch.sum(scores, dim=0)\n",
        "    rowwise_mean = rowwise_sum / mask.sum(-1)\n",
        "    expanded_rowwise_mean = rowwise_mean.unsqueeze(-1).expand(scores.shape)\n",
        "    return expanded_rowwise_mean\n",
        "\n",
        "  def apply_quag(self, attention_scores, mask, l_v, l_t, quads):\n",
        "    if 'VV' in quads:\n",
        "        attention_scores[:, :l_v, :l_v] = self.get_rowwise_average(attention_scores[:, :l_v, :l_v], mask[:, :l_v, :l_v])\n",
        "    if 'VT' in quads:\n",
        "        attention_scores[:, :l_v, -l_t:] = self.get_rowwise_average(attention_scores[:, :l_v, -l_t:], mask[:, :l_v, -l_t:])\n",
        "    if 'TV' in quads:\n",
        "        attention_scores[:, -l_t:, :l_v] = self.get_rowwise_average(attention_scores[:, -l_t:, :l_v], mask[:, -l_t:, :l_v])\n",
        "    if 'TT' in quads:\n",
        "        attention_scores[:, -l_t:, -l_t:] = self.get_rowwise_average(attention_scores[:, -l_t:, -l_t:], mask[:, -l_t:, -l_t:])\n",
        "    attention_scores.masked_fill_(mask==0, 0)\n",
        "    return attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, inputs, mask, l_v, l_t, quads):\n",
        "      # Inputs:\n",
        "      #   inputs: Tensor of shape (batch_size, sequence_length, dim_model)\n",
        "      #   mask: Tensor of shape (batch_size, sequence_length)\n",
        "      #   dim_model: Dimension of the model (e.g., 512)\n",
        "      #   l_v: int    maximum length of video tokens\n",
        "      #   l_t: int    maximum length of question tokens\n",
        "      #   quads: list containing elements from {'VV', 'VT', 'TV', 'TT'}\n",
        "\n",
        "      query = self.q_lin(inputs)\n",
        "      key = self.k_lin(inputs)\n",
        "      value = self.v_lin(inputs)\n",
        "      scaled_dot_product = torch.matmul(query, key.transpose(1, 2)) / math.sqrt(self.dim_model)\n",
        "      mask =  mask.unsqueeze(1) * mask.unsqueeze(2)\n",
        "      scaled_dot_product = scaled_dot_product.masked_fill_(mask==0, -float(\"inf\"))\n",
        "      attention_scores = self.softmax(scaled_dot_product)\n",
        "      attention_scores.masked_fill_(mask==0, 0.0)\n",
        "      print(f\"Attention scores before QUAG\\n{attention_scores}\")\n",
        "      attention_scores = self.apply_quag(attention_scores, mask, l_v, l_t, quads)\n",
        "      print(f\"Attention scores after QUAG {quads}\\n{attention_scores}\")\n",
        "      attended_output = 0.5*(inputs + self.apply_attention_scores(attention_scores, value))\n",
        "      attended_output = self.out_lin(attended_output)\n",
        "      return attended_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of QUAG"
      ],
      "metadata": {
        "id": "ZeZ9giMWAEdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quag_sa = SelfAttentionWithQUAG(6)"
      ],
      "metadata": {
        "id": "BpPrQnaFcFfw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(torch.randn(1,6,6))\n",
        "mask = torch.tensor([1,1,0,1,1,1]).unsqueeze(0)\n",
        "output = quag_sa(input, mask, 3, 3, ['VV', 'TT'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvVR85ihi-YE",
        "outputId": "c9fd6ce5-c29d-4c64-bf62-018f1110759a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores before QUAG\n",
            "tensor([[[0.1253, 0.2028, 0.0000, 0.2995, 0.1659, 0.2065],\n",
            "         [0.1398, 0.2226, 0.0000, 0.2092, 0.1917, 0.2366],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2685, 0.1900, 0.0000, 0.1402, 0.2514, 0.1499],\n",
            "         [0.1633, 0.2167, 0.0000, 0.2429, 0.1852, 0.1919],\n",
            "         [0.1754, 0.2061, 0.0000, 0.2052, 0.1943, 0.2190]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "Attention scores after QUAG ['VV', 'TT']\n",
            "tensor([[[0.1641, 0.1641, 0.0000, 0.2995, 0.1659, 0.2065],\n",
            "         [0.1812, 0.1812, 0.0000, 0.2092, 0.1917, 0.2366],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2685, 0.1900, 0.0000, 0.1805, 0.1805, 0.1805],\n",
            "         [0.1633, 0.2167, 0.0000, 0.2067, 0.2067, 0.2067],\n",
            "         [0.1754, 0.2061, 0.0000, 0.2062, 0.2062, 0.2062]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-87b1fc279acc>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input = torch.tensor(torch.randn(1,6,6))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#QUAG-Attention"
      ],
      "metadata": {
        "id": "IgGW-sghAOc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "XVxUBYWqAQPe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QUAGAttention(nn.Module):\n",
        "  def __init__(self, dim_model):\n",
        "    super().__init__()\n",
        "    self.dim_model = dim_model\n",
        "    self.q_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.k_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.v_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.out_lin = nn.Linear(in_features=dim_model, out_features=dim_model)\n",
        "    self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "  def apply_attention_scores(self, attention_scores, value):\n",
        "    attended_output = torch.matmul(attention_scores, value)\n",
        "    return attended_output\n",
        "\n",
        "  def get_avg_inputs(self, inputs, l_v, l_t, mask, mode):\n",
        "    unmasked_lang = mask[:, -l_t:].sum(1).unsqueeze(1)\n",
        "    unmasked_vid = mask[:, :l_v].sum(1).unsqueeze(1)\n",
        "    ip = inputs * mask.unsqueeze(-1).expand(-1, -1, inputs.shape[-1]) #zero all the values that are padded\n",
        "    #we need to take average of only unpadded values of key\n",
        "    if mode == 'text-vid-avg':\n",
        "      ip = torch.cat(((ip[:,:l_v,:].sum(1)/unmasked_vid).unsqueeze(1),(ip[:,-l_t:,:].sum(1)/unmasked_lang).unsqueeze(1)),1) #(bs, 2, dim)\n",
        "    elif mode == 'vid-avg':\n",
        "      ip = torch.cat(((ip[:,:l_v,:].sum(1)/unmasked_vid).unsqueeze(1),ip[:,-l_t:,:]),1) #(bs, 1+T, dim)\n",
        "    elif mode == 'text-avg':\n",
        "      ip = torch.cat((ip[:,:l_v,:],(ip[:,-l_t:,:].sum(1)/unmasked_lang).unsqueeze(1)),1) #(bs, V+1, dim)\n",
        "    return ip\n",
        "\n",
        "  def get_new_mask(self, mask, l_v, l_t, mode):\n",
        "    if mode == 'text-vid-avg':\n",
        "      attention_mask = torch.cat((mask[:, :, 0].unsqueeze(-1),mask[:, :, l_v].unsqueeze(-1)),-1)\n",
        "    elif mode == 'vid-avg':\n",
        "      attention_mask = torch.cat((mask[:, :, 0].unsqueeze(-1),mask[:, :, -l_t:]),-1)\n",
        "    elif mode == 'text-avg':\n",
        "      attention_mask = torch.cat((mask[:, :, :l_v],mask[:, :, -l_t].unsqueeze(-1)),-1)\n",
        "    return attention_mask\n",
        "\n",
        "\n",
        "  def apply_scaling(self, scaled_dot_product, mask, l_v, l_t, mode):\n",
        "    vid_scaling = math.log(mask[:, :l_v, 0].sum(-1))#math.log(l_v)\n",
        "    text_scaling = math.log(mask[:, -l_t:, 0].sum(-1))#math.log(l_v)\n",
        "    if \"vid\" in mode:\n",
        "      scaled_dot_product[:,:,0] = scaled_dot_product[:,:,0]*vid_scaling\n",
        "    if \"text\" in mode:\n",
        "      scaled_dot_product[:,:,-1] = scaled_dot_product[:,:,-1]*text_scaling\n",
        "    return scaled_dot_product\n",
        "\n",
        "  def forward(self, inputs, mask, l_v, l_t, mode):\n",
        "    # Inputs:\n",
        "    #   inputs: Tensor of shape (batch_size, sequence_length, dim_model)\n",
        "    #   mask: Tensor of shape (batch_size, sequence_length)\n",
        "    #   dim_model: Dimension of the model (e.g., 512)\n",
        "    #   l_v: int    maximum length of video tokens\n",
        "    #   l_t: int    maximum length of question tokens\n",
        "    #   mode: one of {'vid-avg', 'text-avg', 'text-vid-avg'}\n",
        "\n",
        "    average_inputs = self.get_avg_inputs(inputs, l_v, l_t, mask, mode)\n",
        "    query = self.q_lin(inputs)\n",
        "    key = self.k_lin(average_inputs)\n",
        "    value = self.v_lin(average_inputs)\n",
        "\n",
        "    scaled_dot_product = torch.matmul(query, key.transpose(1, 2)) / math.sqrt(self.dim_model)\n",
        "    mask =  mask.unsqueeze(1) * mask.unsqueeze(2)\n",
        "    avg_mask  = self.get_new_mask(mask, l_v, l_t, mode)\n",
        "\n",
        "    scaled_dot_product = scaled_dot_product.masked_fill_(avg_mask==0, -float(\"inf\"))\n",
        "    scaled_dot_product = self.apply_scaling(scaled_dot_product, mask, l_v, l_t, mode)\n",
        "    attention_scores = self.softmax(scaled_dot_product)\n",
        "    attention_scores.masked_fill_(avg_mask==0, 0.0)\n",
        "    print(f\"{mode}-QUAG Attention Matrix:\\n{attention_scores}\")\n",
        "    attended_output = 0.5*(inputs + self.apply_attention_scores(attention_scores, value))\n",
        "    attended_output = self.out_lin(attended_output)\n",
        "    return attended_output"
      ],
      "metadata": {
        "id": "Lx0Ksls4AXM8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of QUAG-attention"
      ],
      "metadata": {
        "id": "gI-BsAVVMpDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quag_attention = QUAGAttention(6)"
      ],
      "metadata": {
        "id": "vtsA1Qf6IK8o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor(torch.randn(1,6,6))\n",
        "mask = torch.tensor([1,1,0,1,1,1]).unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lOt8TP7IljK",
        "outputId": "4d6baa10-53ff-4fa2-ccae-c05aa551b9ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9188e3237a07>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input = torch.tensor(torch.randn(1,6,6))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = quag_attention(input, mask, l_v=4, l_t=2, mode='text-vid-avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrbAFTAIMhoA",
        "outputId": "6052af13-01e3-4030-8f99-3f4b7a613926"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text-vid-avg-QUAG Attention Matrix:\n",
            "tensor([[[0.2407, 0.7593],\n",
            "         [0.3579, 0.6421],\n",
            "         [0.0000, 0.0000],\n",
            "         [0.4135, 0.5865],\n",
            "         [0.4455, 0.5545],\n",
            "         [0.5516, 0.4484]]], grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = quag_attention(input, mask, l_v=4, l_t=2, mode='vid-avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOYUX6KFLQ9_",
        "outputId": "176c7e21-3224-48c1-e177-8aa28b8fadff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vid-avg-QUAG Attention Matrix:\n",
            "tensor([[[0.1110, 0.2332, 0.6558],\n",
            "         [0.1663, 0.2122, 0.6215],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.2564, 0.5068, 0.2368],\n",
            "         [0.2822, 0.4076, 0.3102],\n",
            "         [0.3757, 0.1897, 0.4346]]], grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = quag_attention(input, mask, l_v=4, l_t=2, mode='text-avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6x3AtvoMdBR",
        "outputId": "5b3173b6-7999-4644-d8b7-9c41c33ba8e5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text-avg-QUAG Attention Matrix:\n",
            "tensor([[[0.2652, 0.0837, 0.0000, 0.1850, 0.4661],\n",
            "         [0.4359, 0.0834, 0.0000, 0.1612, 0.3195],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0896, 0.2759, 0.0000, 0.3540, 0.2805],\n",
            "         [0.1444, 0.2523, 0.0000, 0.3247, 0.2787],\n",
            "         [0.3030, 0.3091, 0.0000, 0.1787, 0.2092]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    }
  ]
}